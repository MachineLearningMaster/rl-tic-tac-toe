{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic-Tac-Toe Agent\n",
    "---\n",
    "\n",
    "In this notebook, you will learn to build an RL agent (using Q-learning) that learns to play Numerical Tic-Tac-Toe with odd numbers. The environment is playing randomly with the agent, i.e. its strategy is to put an even number randomly in an empty cell. \n",
    "\n",
    "The following is the layout of the notebook:\n",
    "    - Defining epsilon-greedy strategy\n",
    "    - Tracking state-action pairs for convergence\n",
    "    - Define hyperparameters for the Q-learning algorithm\n",
    "    - Generating episode and applying Q-update equation\n",
    "    - Checking convergence in Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnvironment:\n",
    "    def __init__(self):\n",
    "        \"\"\"initialise the board\"\"\"\n",
    "\n",
    "        # initialise state as an array\n",
    "        self.state = [np.nan for _ in range(9)]\n",
    "\n",
    "        # all possible numbers\n",
    "        self.all_possible_numbers = [i for i in range(1, len(self.state) + 1)]\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def is_winning(self, curr_state):\n",
    "        \"\"\"Takes state as an input and returns whether any row, column or diagonal has winning sum\n",
    "        Example: Input state- [1, 2, 3, 4, nan, nan, nan, nan, nan]\n",
    "        Output = False\"\"\"\n",
    "\n",
    "        # define possible index collections in each of (horizontal, vertical, diagonal) directions\n",
    "\n",
    "        # 3 horizontal rows of a 3 X 3 playing board\n",
    "        horizontal_indices = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "\n",
    "        # 3 vertical rows of a 3 X 3 playing board\n",
    "        vertical_indices = [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n",
    "\n",
    "        # 2 diagonal rows : (top left --> bottom right) & (top right --> bottom left)\n",
    "        diagonal_indices = [[0, 4, 8], [2, 4, 6]]\n",
    "\n",
    "        # sum across each group of indices should be equal to 15 to win the game\n",
    "        horizontal_sum = [\n",
    "            np.sum(np.array(curr_state)[i]) for i in horizontal_indices\n",
    "        ]\n",
    "        vertical_sum = [\n",
    "            np.sum(np.array(curr_state)[i]) for i in vertical_indices\n",
    "        ]\n",
    "        diagonal_sum = [\n",
    "            np.sum(np.array(curr_state)[i]) for i in diagonal_indices\n",
    "        ]\n",
    "\n",
    "        horizontal_win = list(filter(lambda x: x == 15, horizontal_sum))\n",
    "        vertical_win = list(filter(lambda x: x == 15, vertical_sum))\n",
    "        diagonal_win = list(filter(lambda x: x == 15, diagonal_sum))\n",
    "\n",
    "        #  game is won if sum across any direction is equal to 15\n",
    "        if len(horizontal_win) != 0 or len(vertical_win) != 0 or len(\n",
    "                diagonal_win) != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_terminal(self, curr_state):\n",
    "        # Terminal state could be winning state or when the board is filled up\n",
    "\n",
    "        if self.is_winning(curr_state) == True:\n",
    "            return True, \"Win\"\n",
    "\n",
    "        elif len(self.allowed_positions(curr_state)) == 0:\n",
    "            return True, \"Tie\"\n",
    "\n",
    "        else:\n",
    "            return False, \"Resume\"\n",
    "\n",
    "    def allowed_positions(self, curr_state):\n",
    "        \"\"\"Takes state as an input and returns all indexes that are blank\"\"\"\n",
    "        return [i for i, val in enumerate(curr_state) if np.isnan(val)]\n",
    "\n",
    "    def allowed_values(self, curr_state):\n",
    "        \"\"\"Takes the current state as input and returns all possible (unused) values that can be placed on the board\"\"\"\n",
    "\n",
    "        # fetch all allowed values used in the game\n",
    "        used_values = [val for val in curr_state if not np.isnan(val)]\n",
    "\n",
    "        # RL agent is only allowed to play odd numbers : {1,3,5,7,9}\n",
    "        # fetch numbers which an agent can still play i.e.\n",
    "        # odd numbers which have not been played by the agent so far\n",
    "        agent_values = [\n",
    "            val for val in self.all_possible_numbers\n",
    "            if val not in used_values and val % 2 != 0\n",
    "        ]\n",
    "\n",
    "        # environment is only allowed to play even numbers : {2,4,6,8}\n",
    "        # fetch numbers which environment can still play i.e.\n",
    "        # even numbers which have not been played by the environment so far\n",
    "        env_values = [\n",
    "            val for val in self.all_possible_numbers\n",
    "            if val not in used_values and val % 2 == 0\n",
    "        ]\n",
    "\n",
    "        return (agent_values, env_values)\n",
    "\n",
    "    def action_space(self, curr_state):\n",
    "        \"\"\"Takes the current state as input and returns all possible actions, i.e, all combinations of allowed positions and allowed values\"\"\"\n",
    "\n",
    "        allowed_positions = self.allowed_positions(curr_state)\n",
    "        allowed_values = self.allowed_values(curr_state)\n",
    "\n",
    "        # action space of a given space is the cartesian product of all allowed positions and allowed values\n",
    "        agent_actions = product(allowed_positions, allowed_values[0])\n",
    "        env_actions = product(allowed_positions, allowed_values[1])\n",
    "\n",
    "        return (agent_actions, env_actions)\n",
    "\n",
    "    def state_transition(self, curr_state, curr_action):\n",
    "        \"\"\"Takes current state and action and returns the board position just after agent's move.\n",
    "        Example: Input state- [1, 2, 3, 4, nan, nan, nan, nan, nan], action- [7, 9] or [position, value]\n",
    "        Output = [1, 2, 3, 4, nan, nan, nan, 9, nan]\n",
    "        \"\"\"\n",
    "\n",
    "        # current new state variable from existing state\n",
    "        new_state = [i for i in curr_state]\n",
    "\n",
    "        # update current action\n",
    "        new_state[curr_action[0]] = curr_action[1]\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def step(self, curr_state, curr_action):\n",
    "        \"\"\"Takes current state and action and returns the next state, reward and whether the state is terminal. Hint: First, check the board position after\n",
    "        agent's move, whether the game is won/loss/tied. Then incorporate environment's move and again check the board status.\n",
    "        Example: Input state- [1, 2, 3, 4, nan, nan, nan, nan, nan], action- [7, 9] or [position, value]\n",
    "        Output = ([1, 2, 3, 4, nan, nan, nan, 9, nan], -1, False)\"\"\"\n",
    "\n",
    "        # generate new state after agent's move\n",
    "        new_state = self.state_transition(curr_state, curr_action)\n",
    "\n",
    "        # check if terminal state has been reached i.e.\n",
    "        # either agent has won or it's a tie\n",
    "        has_reached_terminal_state, message = self.is_terminal(new_state)\n",
    "\n",
    "        if has_reached_terminal_state:\n",
    "            # set correct reward and message when game proceeds to a terminal state due to agent move\n",
    "            if message == \"Win\":\n",
    "                reward = 10\n",
    "                game_message = \"Agent Won!\"\n",
    "            else:\n",
    "                reward = 0\n",
    "                game_message = \"It's a tie!\"\n",
    "\n",
    "            return (new_state, reward, has_reached_terminal_state,\n",
    "                    game_message)\n",
    "        else:\n",
    "            # game is not in terminal state\n",
    "\n",
    "            # generate random environment action\n",
    "            _, env_actions = self.action_space(new_state)\n",
    "            env_action = random.choice(\n",
    "                [ac for i, ac in enumerate(env_actions)])\n",
    "\n",
    "            # move to new state due to environment action\n",
    "            new_state_post_env_action = self.state_transition(\n",
    "                new_state, env_action)\n",
    "\n",
    "            # check if environment action results in a terminal state\n",
    "            has_reached_terminal_state, message = self.is_terminal(\n",
    "                new_state_post_env_action)\n",
    "\n",
    "            # decide whether environment has won, it's a tie or game can continue further\n",
    "            if has_reached_terminal_state:\n",
    "                if message == \"Win\":\n",
    "                    reward = -10\n",
    "                    game_message = \"Environment Won!\"\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    game_message = \"It's a tie!\"\n",
    "            else:\n",
    "                reward = -1\n",
    "                game_message = \"Resume\"\n",
    "\n",
    "            return (\n",
    "                new_state_post_env_action,\n",
    "                reward,\n",
    "                has_reached_terminal_state,\n",
    "                game_message,\n",
    "            )\n",
    "\n",
    "    def reset(self):\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment object\n",
    "env = TicTacToeEnvironment()\n",
    "\n",
    "# q-dictionary used for tracking (state, action) pairs with corresponing q-values\n",
    "Q_dict = collections.defaultdict(dict)\n",
    "\n",
    "# set of states tracked for convergence\n",
    "tracked_states = collections.defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert state array into a string to be used as a key in Q - Dictionary\n",
    "\n",
    ">  for a given state :\n",
    "-  x | 4 | 5\n",
    "-  3 | 8 | x\n",
    "-  x | x | x\n",
    "   \n",
    "*generated key :* **x-4-5-3-8-x-x-x-x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_state(state):\n",
    "\n",
    "    return ('-'.join(str(e) for e in state)).replace('nan', 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate valid 'agent' actions for a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_actions(state):\n",
    "    \n",
    "    possible_agent_actions, possible_env_actions = env.action_space(state)\n",
    "\n",
    "    agent_actions_list = [ac for i, ac in enumerate(possible_agent_actions)]\n",
    "\n",
    "    return agent_actions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add new state to Q - Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dict(state):\n",
    "    # convert new state into Q-dictionary key form\n",
    "    new_state_key = Q_state(state)\n",
    "\n",
    "    # calculate valid actions for the given state\n",
    "    agent_actions = valid_actions(state)\n",
    "\n",
    "    if new_state_key not in Q_dict.keys():\n",
    "        for action in agent_actions:\n",
    "            Q_dict[new_state_key][action] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tracking states initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_tracking_states():\n",
    "    state_action_pair = [('x-7-3-x-2-8-x-5-x', (0, 1)),\n",
    "                         ('x-1-2-x-7-x-x-6-x', (3, 3)),\n",
    "                         ('4-x-2-x-5-3-x-x-1', (7, 7)),\n",
    "                         ('6-x-7-x-x-1-x-x-4', (1, 5)),\n",
    "                         ('x-x-x-4-9-2-7-x-x', (1, 1)),\n",
    "                         ('x-x-3-8-2-x-x-x-5', (7, 1)),\n",
    "                         ('3-x-7-x-x-x-x-8-2', (5, 5)),\n",
    "                         ('2-x-1-8-x-7-x-x-x', (4, 3))]\n",
    "    for st, ac in state_action_pair:\n",
    "        tracked_states[st][ac] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### persisting q-values corresponding to tracked states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tracking_states():\n",
    "    for state in tracked_states.keys():\n",
    "        for action in tracked_states[state].keys():\n",
    "            if state in Q_dict and action in Q_dict[state]:\n",
    "                tracked_states[state][action].append(Q_dict[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 600000\n",
    "\n",
    "learning_rate=0.01\n",
    "gamma = 0.91\n",
    "\n",
    "checkpoint_state_tracking = 1000\n",
    "checkpoint_print_episodes = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
